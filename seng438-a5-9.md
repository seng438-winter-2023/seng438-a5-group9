**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:         | 9     |
| --------------    | ---   |
| Student Names     |       |
| Saman Pordanesh    |       |
| Bryant Stevnes    |       |
| Noureldin Amer    |       |
| Thevin Mahawatte  |       |

# Introduction

The main goal of this lab was to assist students in comprehending the assessment of dependability and the use of tools such as C-SFRAT and the Reliability Demonstration Chart. Furthermore, the lab delved into failure reliability growth testing, allowing for the examination of changes made to a product over a specific time frame. Finally, the group also discovered the importance of failure data analysis in determining approaches to prevent failures. These concepts reinforced the knowledge students had acquired in the lectures and provided the group with a comprehensive understanding of the critical role of reliability testing in software testing procedures.

# 

# Assessment Using Reliability Growth Testing 
![1](https://user-images.githubusercontent.com/74850874/230690688-c3ddeb07-f303-41f5-a4a9-154734bba16b.png)
This lab's primary purpose was to help students grasp the evaluation of reliability and the application of tools like C-SFRAT and the Reliability Demonstration Chart. Additionally, the lab investigated failure reliability growth testing, enabling the analysis of alterations made to a product during a particular period. The group also learned the significance of failure data analysis for devising strategies to avoid failures. These ideas strengthened the concepts students had learned in the lectures, giving the group a thorough understanding of the vital importance of reliability testing in software testing processes.

![2](https://user-images.githubusercontent.com/74850874/230690691-ad002ee8-a2c3-469e-a7f6-1a21689026fd.png)
The above figure was obtained using C-SFRAT by modeling the data that best represented the given parameters to illustrate the failure data. It demonstrates the relationship between Geometric and IFR models within the ranges of 2 to 10 and 22 to 26. In the higher interval, the IFRGSB model more accurately follows the data compared to the Geometric model, while the Geometric model outperforms the IFRGSB model in the lower data interval. This indicates that various models perform better for different ranges. Furthermore, it highlights the necessity of incorporating all three parameters to obtain the most accurate models that depict the provided failure data.

![3](https://user-images.githubusercontent.com/74850874/230690692-57196ba4-0d80-4a43-9f3b-cb2fe6ea7fcc.png)
The figure illustrates the data failure rate of the selected models (IFRGSB and Geometric) using the time between intervals as the parameter, which optimally represents the data failure rate based on the given parameters.

Geometric and IFRGSB were chosen as the models because they provided the most accurate visual representation of the failure data we were given. This data was also plotted using C-SFRAT. It is evident that the models exhibit greater accuracy in higher data intervals compared to lower ones.

![4](https://user-images.githubusercontent.com/74850874/230690693-e21078d0-2d5b-45e3-878a-022150caab8a.png)
The graph displays trends based on all three parameters, revealing that the models are more accurate for higher time intervals. The models do not accurately represent the failure data at lower data intervals, but they perform reasonably well in intervals 8-14.

In addition to highlighting the differences in failure data representation, the graph demonstrates why selecting all three parameters is the best approach. It also illustrates why the Geometric model is more suitable for lower time frames and why IFRGSB is better for higher time frames. This graph confirms the accuracy of the chosen models, as they more clearly mirror the failure data.

![5](https://user-images.githubusercontent.com/74850874/230690694-a9b9767e-d692-4cdb-bb7e-8aea8d9af99a.png)
We performed a Laplace test on the failure data, which provided us with a deeper understanding of the importance of reliability growth for different y values. As a result, the graph depicted below supports our assumption of having an accurate model from intervals 8 to 14.

One benefit of reliability growth analysis is its ability to forecast potential failures in the future. This can contribute to cost savings by addressing these issues during the development process or after the software has been completed. Additionally, it aids in predicting the system's accuracy over time.


# Assessment Using Reliability Demonstration Chart 

![6](https://user-images.githubusercontent.com/74850874/230691020-cee8a347-e819-4f63-8be2-93a28893f968.png)
Referring to the RDC1 figure in the RDC folder, with a developer and customer risk tolerance of 1 failure every 10 seconds and a discrimination ratio of 2, the system will be accepted with a Failure Intensity Objective of 3 failures per second. Thus, the customer and developer require the software to have a maximum of 1 failure every 10 seconds.

The discrimination ratio's purpose is to indicate the amount of error allowed in the calculation, which corresponds to the area between the acceptance and rejection lines. To be considered acceptable for developers and customers, the software must achieve a Failure Intensity Objective of at least 3 failures per second, which is deemed satisfactory.

![7](https://user-images.githubusercontent.com/74850874/230691021-24b8cc92-ee37-4b7d-9779-fe5f2f718422.png)
The RDC2 figure illustrates the effect of halving the Failure Intensity Objective (FIO). In the continuous part of the graph, the FIO data neither passes nor fails the risk requirements, as it is undetermined due to the allowable error generated by the discrimination ratio. For example, if the discrimination ratio were to be reduced, the failure data would be rejected under that circumstance.

![89](https://user-images.githubusercontent.com/74850874/230691024-c45d9e92-b393-42a0-92f1-6199c1adcfe8.png)
The RDC3 figure demonstrates the outcome when the Failure Intensity Objective (FIO) is doubled from its original value. With an FIO of 6, the failure data significantly surpasses the acceptance rate, ensuring that it more than satisfies the requirements of both customers and developers.

# 

# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques

Both Reliability Growth Testing (RGT) and Reliability Demonstration Chart (RDC) are techniques used in software integration testing to assess the failure data and improve the reliability of software systems.

Here are some similarities we identified,
  1.	Reliability Growth Testing is used to identify and correct defects in software that affect reliability, while Reliability Demonstration Chart is used to        demonstrate that a software system meets a specified reliability goal.
  2.	Both use statistical methods to analyze data and make predictions about the reliability of the software system. In Reliability Growth Testing, statistical models are used to predict the number of remaining defects, while in Reliability Demonstration Chart, statistical methods are used to estimate the probability of meeting the specified reliability goal.
  3.	Both techniques require iterative testing to provide a better result of the system under test.

Here are some differences we identified,
  1.	Reliability growth testing is used as iterative testing and analysis of a software system to identify and eliminate defects that cause failures while Reliability Demonstration Chart technique is used to demonstrate the reliability of a software system based on a predefined level of confidence.
  2.	The goal of reliability growth testing technique is to improve the reliability of the software system over time and the goal of reliability demonstration chart testing technique is to provide evidence that the software system meets a specified level of reliability.
  3.	What is reliability growth testing mostly does is that it monitors and analyze failure data, identify the root cause of failures, and implement corrective actions to eliminate the identified defects while reliability demonstration chart does is that it develops a reliability demonstration plan, select a statistical distribution to model the failure data, and construct an while reliability demonstration chart to assess the software system's reliability.

# How the team work/effort was divided and managed

For this assignment, we worked by diving each technique with each other and started working on it while explaining to others how each technique does and what kind o results it provides. Finally for the report, 8 topics we had were divided between each team member as 2 topics per person.

# Difficulties encountered, challenges overcome, and lessons learned
Our group, encountered various challenges during this lab. The primary objective was to delve into diverse methodologies for evaluating software reliability, including reliability growth testing and the reliability demonstration chart (RDC). By examining these techniques, we were able to gain distinct insights into the approaches for assessing a system's reliability.

The most significant hurdle we faced stemmed from the tools required for the lab. Mastering these tools proved to be a steep learning curve for the team. Furthermore, some group members experienced device compatibility issues, which prevented them from running the executables for specific programs. This added an extra layer of complexity to the task at hand.

Additionally, we found the RDC Excel file to be somewhat challenging to navigate. The file contained numerous variables and values that lacked detailed explanations, leaving us uncertain about which parameters we were permitted to modify. As a result, our group had to engage in an iterative process of trial-and-error to establish the appropriate Mean Time To Failure (MTTF) value.

Through the process of overcoming these challenges, we learned valuable lessons in perseverance, collaboration, and problem-solving. We honed our skills in utilizing various software reliability testing methods and deepened our understanding of their respective merits. Moreover, we developed a strong sense of teamwork by leaning on each other's strengths and assisting one another in overcoming the obstacles we faced.




# Comments/feedback on the lab itself
A more extensive tutorial (preferably video tutuorial), would be recommended. Overall, the TAs were very helpful.
